\chapter{Power-series methods} \label{ps:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Power series}
\label{powerseries:section}

%mbxINTROSUBSECTION

\sectionnotes{1.5 or 2 lectures\EPref{, \S8.1 in \cite{EP}}\BDref{,
\S5.1 in \cite{BD}}}

Many functions can be written in terms of a power series
\begin{equation*}
\sum_{k=0}^\infty a_k {(x-x_0)}^k .
\end{equation*}
If we assume that a solution of a differential equation is written as a
power series, then perhaps we can use a method reminiscent of undetermined
coefficients.  That is, we will try to solve for the numbers $a_k$.
Before we carry out this process, we review some results
and concepts about power series.

\subsection{Definition}

As we said, a \emph{\myindex{power series}} is an expression such as
\begin{equation} \label{ps:sereq1}
\sum_{k=0}^\infty a_k {(x-x_0)}^k =
a_0 + 
a_1 (x-x_0) +
a_2 {(x-x_0)}^2 +
a_3 {(x-x_0)}^3 + \cdots,
\end{equation}
where $a_0,a_1,a_2,\ldots,a_k,\ldots$ and $x_0$ are constants.  Let
\begin{equation*}
S_n(x) = \sum_{k=0}^n a_k {(x-x_0)}^k =
a_0 + a_1 (x-x_0) + a_2 {(x-x_0)}^2 + a_3 {(x-x_0)}^3 + \cdots + a_n {(x-x_0)}^n ,
\end{equation*}
denote the so-called \emph{\myindex{partial sum}}.  If for some $x$,
the limit
\begin{equation*}
\lim_{n\to \infty} S_n(x) = \lim_{n\to\infty} \sum_{k=0}^n a_k {(x-x_0)}^k
\end{equation*}
exists, we say the series \eqref{ps:sereq1}
\emph{converges}\index{convergence of a power series} at $x$.
At $x=x_0$, the series always converges to $a_0$.
When \eqref{ps:sereq1}
converges at any other $x \not= x_0$,
we say \eqref{ps:sereq1} is a
\emph{\myindex{convergent power series}}, and we write
\begin{equation*}
\sum_{k=0}^\infty a_k {(x-x_0)}^k = 
\lim_{n\to\infty} \sum_{k=0}^n a_k {(x-x_0)}^k.
\end{equation*}
If the series does not converge for any point $x \not= x_0$, we say that
the series is \emph{divergent}\index{divergent power series}.

\begin{example} \label{ps:expex}
The series
\begin{equation*}
\sum_{k=0}^\infty \frac{1}{k!} x^k = 
1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \cdots
\end{equation*}
is convergent for any $x$.
Recall that $k! = 1\cdot 2\cdot 3 \cdots k$ is the
factorial.  By convention we define $0! = 1$.
You may recall that this series
converges to $e^x$.
\end{example}

We say that \eqref{ps:sereq1}
\emph{\myindex{converges absolutely}}\index{absolute convergence}
at $x$ whenever the limit
\begin{equation*}
\lim_{n\to\infty} \sum_{k=0}^n
\lvert a_k \rvert \, {\lvert x-x_0 \rvert}^k 
\end{equation*}
exists.  That is, the series
$\sum_{k=0}^\infty \lvert a_k \rvert \, {\lvert x-x_0 \rvert}^k$
is convergent.
If \eqref{ps:sereq1} converges absolutely at $x$, then it
converges at $x$.  However, the opposite implication is not true.

\begin{example} \label{ps:1kex}
The series
\begin{equation*}
\sum_{k=1}^\infty \frac{1}{k} x^k
\end{equation*}
converges absolutely for all $x$ in the interval $(-1,1)$.
It converges at $x=-1$,
as
$\sum_{k=1}^\infty \frac{{(-1)}^k}{k}$ converges (conditionally)
by the alternating series
test.
The power series does not converge absolutely at $x=-1$, because
$\sum_{k=1}^\infty \frac{1}{k}$ does not converge.
The series
diverges at $x=1$.
\end{example}

\subsection{Radius of convergence}

If a power series converges absolutely
at some $x_1$, then for all $x$ such that
$\lvert x - x_0  \rvert \leq \lvert x_1 - x_0 \rvert$ (that is, $x$ is
closer than $x_1$ to $x_0$) we have
$\lvert a_k {(x-x_0)}^k \rvert \leq
\lvert a_k {(x_1-x_0)}^k \rvert$
for all $k$.
As the numbers $\lvert a_k {(x_1-x_0)}^k \rvert$ sum to some finite
limit, summing smaller positive numbers
$\lvert a_k {(x-x_0)}^k \rvert$ must also have a finite limit.
Hence, the series must converge
absolutely at $x$. %  We have the following result.

\begin{theorem}
For a power series \eqref{ps:sereq1}, there exists a number
$\rho$ (we allow $\rho=\infty$)
called the \emph{\myindex{radius of convergence}} such that
the series converges absolutely on the interval
$(x_0-\rho,x_0+\rho)$ and diverges for $x < x_0-\rho$ and $x > x_0+\rho$.
We write $\rho=\infty$ if
the series converges for all $x$.
\end{theorem}

\begin{myfig}
\capstart
\inputpdft{ps-conv}
\caption{Convergence of a power series.\label{ps:convfig}}
\end{myfig}

See \figurevref{ps:convfig}.
In \exampleref{ps:expex}, the radius of convergence is $\rho = \infty$
as the series converges everywhere.
In \exampleref{ps:1kex}, the radius of convergence is $\rho=1$.
We note that $\rho = 0$ is another way of saying that the series is
divergent.

A useful test for convergence of a series is the
\emph{ratio test}\index{ratio test for series}.  Suppose that
\begin{equation*}
\sum_{k=0}^\infty c_k
\end{equation*}
is a series and the limit
\begin{equation*}
L = \lim_{k\to\infty} \left \lvert \frac{c_{k+1}}{c_k} \right \rvert
\end{equation*}
exists.  Then the series converges absolutely if $L < 1$ and diverges
if $L > 1$.

We apply this test to the series \eqref{ps:sereq1}. 
Let $c_k = a_k {(x - x_0)}^k$ in the test.  Compute
\begin{equation*}
L = \lim_{k\to\infty} \left \lvert \frac{c_{k+1}}{c_k} \right \rvert
=
\lim_{k\to\infty} \left \lvert
\frac{a_{k+1} {(x - x_0)}^{k+1}}{a_k {(x - x_0)}^k}
\right \rvert
=
\lim_{k\to\infty} \left \lvert
\frac{a_{k+1}}{a_k}
\right \rvert
\lvert  x - x_0 \rvert .
\end{equation*}
Define $A$ by
\begin{equation*}
A =
\lim_{k\to\infty} \left \lvert
\frac{a_{k+1}}{a_k}
\right \rvert .
\end{equation*}
Then the series \eqref{ps:sereq1}
converges absolutely if $1 > L = A \lvert x - x_0 \rvert$.
If $A > 0$, then
the series converges absolutely
if $\lvert x - x_0 \rvert < \nicefrac{1}{A}$,
and diverges if $\lvert x - x_0 \rvert > \nicefrac{1}{A}$.  That is,
the radius of convergence is $\nicefrac{1}{A}$.
If $A = 0$, then the series always converges.

A similar test is the \emph{root test}\index{root test for series}.
Suppose
\begin{equation*}
L = \lim_{k\to\infty} \sqrt[k]{\lvert c_k \rvert}
\end{equation*}
exists.  Then $\sum_{k=0}^\infty c_k$ converges absolutely if $L < 1$
and diverges if $L > 1$.  We can use the same calculation as above
to find $A$.
Let us summarize.

\begin{theorem}[Ratio and root tests for power series]
Consider a power series
\begin{equation*}
\sum_{k=0}^\infty a_k {(x-x_0)}^k
\end{equation*}
such that
\begin{equation*}
A =
\lim_{k\to\infty}
\left \lvert
\frac{a_{k+1}}{a_k}
\right \rvert
\qquad \text{or} \qquad
A =
\lim_{k\to\infty} \sqrt[k]{\lvert a_k \rvert}
\end{equation*}
exists.  If $A = 0$, then the radius of convergence of the series
is $\infty$.  Otherwise, the radius of convergence is $\nicefrac{1}{A}$.
\pagebreak[3]
\end{theorem}

\begin{example}
Suppose we have the series
\begin{equation*}
\sum_{k=0}^\infty 2^{-k} {(x-1)}^k .
\end{equation*}
We compute the limit in the ratio test,
\begin{equation*}
A = \lim_{k\to\infty} 
\left \lvert
\frac{a_{k+1}}{a_k}
\right \rvert
=
\lim_{k\to\infty} 
\left \lvert
\frac{2^{-k-1}}{2^{-k}}
\right \rvert
=
\lim_{k\to\infty} 
2^{-1} = \nicefrac{1}{2}.
\end{equation*}
Therefore, the radius of convergence is $2$, and the series
converges absolutely on the interval $(-1,3)$.
We could just as well have used the root test:
\begin{equation*}
A = 
\lim_{k\to\infty} 
\sqrt[k]{\lvert
a_k
\rvert}
=
\lim_{k\to\infty} 
\sqrt[k]{\lvert
2^{-k}
\rvert}
=
\lim_{k\to\infty} 
2^{-1}
=
\nicefrac{1}{2}.
\end{equation*}
\end{example}

\begin{example}
Consider
\begin{equation*}
\sum_{k=0}^\infty \frac{1}{k^k} {x}^k .
\end{equation*}
Compute the limit for the root test,
\begin{equation*}
A =
\lim_{k\to\infty} 
\sqrt[k]{\lvert a_k \rvert}
=
\lim_{k\to\infty} 
\sqrt[k]{
\left\lvert\frac{1}{k^k}\right\rvert}
=
\lim_{k\to\infty} 
\sqrt[k]{
{\left\lvert\frac{1}{k}\right\rvert}^{k}}
=
\lim_{k\to\infty} 
\frac{1}{k}
=
0 .
\end{equation*}
So the radius of convergence is $\infty$: The series
converges everywhere.  The ratio test would also work here.
\end{example}

The root or the ratio test as given does not always apply.  That is,
the limit
of
$\bigl \lvert \frac{a_{k+1}}{a_k} \bigr \rvert$
or
$\sqrt[k]{\lvert a_k \rvert}$
might not exist.
There exist more sophisticated ways of finding the radius of convergence,
but those would be beyond the scope of this chapter.  The two methods above
cover many of the series that arise in practice.  Often if the root test
applies, so does the ratio test, and vice versa, though the limit might
be easier to compute in one way than the other.

\subsection{Analytic functions}

Functions represented by power series are called
\emph{\myindex{analytic functions}}.  Not every function is analytic,
although the majority of the functions you have seen in calculus are.
An analytic function $f(x)$ is equal to its \emph{\myindex{Taylor series}}%
\footnote{Named after the English mathematician
\href{http://en.wikipedia.org/wiki/Brook_Taylor}{Sir Brook Taylor}
(1685--1731).} (a power series computed from $f$)
near a point $x_0$.
That is, for $x$ near $x_0$,
\begin{equation} \label{ps:tayloreq}
f(x) = \sum_{k=0}^\infty \frac{f^{(k)}(x_0)}{k!} {(x-x_0)}^k 
=
f(x_0)
+ f'(x_0) (x-x_0)
+ \frac{f''(x_0)}{2} (x-x_0)^2
+ \cdots
,
\avoidbreak
\end{equation}
where $f^{(k)}(x_0)$ denotes the $k^{\text{th}}$ derivative of $f(x)$
at the point $x_0$.

For example, sine is an analytic function and its Taylor series
around $x_0 = 0$
is given by
\begin{equation*}
\sin(x) = \sum_{n=0}^\infty \frac{{(-1)}^n}{(2n+1)!}
 x^{2n+1} .
\end{equation*}
In \figurevref{ps:sin}, we plot $\sin(x)$ and the truncations of the
series up to degree 5 and 9.  You can see that the approximation is very
good for $x$ near 0, but gets worse for $x$ further away from 0.  This is 
what happens in general.
To get a good approximation far away from $x_0$ you
need to take more and more terms of the Taylor series.

\begin{myfig}
\capstart
\diffyincludegraphics{width=3in}{width=4.5in}{ps-sin}
\caption{The sine function and its Taylor approximations
around $x_0=0$
of $5^{\text{th}}$ and $9^{\text{th}}$ degree.\label{ps:sin}}
\end{myfig}

\subsection{Manipulating power series}

One of the main properties of power series that we will use is
that we can differentiate them term by term.  That is,
suppose that 
$\sum a_k {(x-x_0)}^k$ is a convergent power series.  Then
for $x$ in the radius of convergence, we have
\begin{equation*}
\frac{d}{dx}
\left[\sum_{k=0}^\infty a_k {(x-x_0)}^k\right]
=
\sum_{k=1}^\infty k a_k {(x-x_0)}^{k-1}
=
a_1
+ 2 a_2 (x-x_0)
+ 3 a_3 {(x-x_0)}^{2}
+ \cdots
.
\end{equation*}
Notice that the term corresponding to $k=0$ disappeared as
it was constant.  The radius of convergence of the differentiated
series is the same as that of the original.

\begin{example}
Let us show that the exponential $y=e^x$ solves $y'=y$.  Suppose
we didn't know that.
Write
\begin{equation*}
y = e^x = \sum_{k=0}^\infty \frac{1}{k!} x^k .
\end{equation*}
Now differentiate
\begin{equation*}
y' = \sum_{k=1}^\infty k \frac{1}{k!} x^{k-1} =
\sum_{k=1}^\infty \frac{1}{(k-1)!} x^{k-1} .
\end{equation*}
We \emph{reindex}\index{reindexing the series}
the series by simply replacing $k$ with $k+1$.  The series
does not change, what changes is simply how we write it.  After
reindexing the series starts 
at $k=0$ again.
\begin{equation*}
\sum_{k=1}^\infty \frac{1}{(k-1)!} x^{k-1} =
\sum_{k+1=1}^\infty \frac{1}{\bigl((k+1)-1\bigr)!} x^{(k+1)-1} =
\sum_{k=0}^\infty \frac{1}{k!} x^k .
\end{equation*}
That was precisely the power series for $e^x$ we started with,
so we showed that $\frac{d}{dx} [ e^x ] = e^x$.
\end{example}

Convergent power series can be added and multiplied together, and multiplied
by constants using the following rules.  First, we can add series by
adding term by term,
\begin{equation*}
\left(\sum_{k=0}^\infty a_k {(x-x_0)}^k\right)
+
\left(\sum_{k=0}^\infty b_k {(x-x_0)}^k\right)
=
\sum_{k=0}^\infty (a_k+b_k) {(x-x_0)}^k .
\end{equation*}
We can multiply by constants,
\begin{equation*}
\alpha
\left(\sum_{k=0}^\infty a_k {(x-x_0)}^k\right)
=
\sum_{k=0}^\infty \alpha a_k {(x-x_0)}^k .
\end{equation*}
We can also multiply series together,
\begin{equation*}
\left(\sum_{k=0}^\infty a_k {(x-x_0)}^k\right)
\,
\left(\sum_{k=0}^\infty b_k {(x-x_0)}^k\right)
=
\sum_{k=0}^\infty c_k {(x-x_0)}^k ,
\end{equation*}
where
$c_k = a_0b_k + a_1 b_{k-1} + \cdots + a_k b_0$.
The radius of convergence of the sum or the product
is at least the minimum of the radii of convergence of
the two series involved.

\subsection{Power series for rational functions}

Polynomials are simply finite power series.  That is, a polynomial
is a power series where
the $a_k$ are zero for all $k$ large enough.  We can always expand
a polynomial as a power series about any point $x_0$ by writing
the polynomial as a polynomial in $(x-x_0)$.  For example,
let us write
$2x^2-3x+4$ as a power series around $x_0 = 1$:
\begin{equation*}
2x^2-3x+4 = 3 + (x-1) + 2{(x-1)}^2 .
\end{equation*}
In other words, $a_0 = 3$, $a_1 = 1$, $a_2 = 2$, and all other
$a_k = 0$.  To do this, we know that $a_k = 0$ for all $k \geq 3$ as the
polynomial is of degree 2.
We write $a_0 + a_1(x-1) + a_2{(x-1)}^2$, we expand, and we solve
for $a_0$, $a_1$, and $a_2$.  We could have also differentiated at $x=1$
and used the Taylor series formula \eqref{ps:tayloreq}.

Let us look at rational functions, that is, ratios of polynomials.
An important fact is 
that a series for a function only defines the function
on an interval even if the function is defined elsewhere.  For example, for
$-1 < x < 1$,
\begin{equation*}
\frac{1}{1-x} =
\sum_{k=0}^\infty x^k =
1 + x + x^2 + \cdots
\end{equation*}
This series is called the \emph{\myindex{geometric series}}.  The ratio
test tells us that the radius of convergence is $1$.  The series
diverges for $x \leq -1$ and $x \geq 1$, even though
$\frac{1}{1-x}$ is defined for all $x \not= 1$.

We can use the geometric series together with rules for addition and
multiplication of power series to expand rational functions around
a point, as long as the denominator is not zero at $x_0$.  Note that
as for polynomials, we could
equivalently use the Taylor series expansion \eqref{ps:tayloreq}.

\begin{example}
Expand $\frac{x}{1+2x+x^2}$ as a power series around the origin ($x_0 = 0$) and
find the radius of convergence.

First, write $1+2x+x^2 = {(1+x)}^2 = {\bigl(1-(-x)\bigr)}^2$.
Compute
\begin{equation*}
\begin{split}
\frac{x}{1+2x+x^2}
&=
x \,
{\left(
\frac{1}{1-(-x)}
\right)}^2
\\
&=
x \,
{ \left( 
\sum_{k=0}^\infty {(-1)}^k x^k 
\right)}^2
\\
&=
x \,
\left(
\sum_{k=0}^\infty c_k x^k 
\right)
\\
&=
\sum_{k=0}^\infty c_k x^{k+1} ,
\end{split}
\end{equation*}
where to get $c_k$, we use the formula for the product of series:
$c_0 = 1$, $c_1 = -1 -1 = -2$, $c_2 = 1+1+1 = 3$, etc.
Therefore
\begin{equation*}
\frac{x}{1+2x+x^2}
=
\sum_{k=1}^\infty {(-1)}^{k+1} k x^k
= x-2x^2+3x^3-4x^4+\cdots
\end{equation*}
The radius of convergence is at least 1.  We use the ratio test
\begin{equation*}
\lim_{k\to\infty}
\left\lvert \frac{a_{k+1}}{a_k} \right\rvert
=
\lim_{k\to\infty}
\left\lvert \frac{{(-1)}^{k+2} (k+1)}{{(-1)}^{k+1}k} \right\rvert
=
\lim_{k\to\infty}
\frac{k+1}{k}
= 1 .
\end{equation*}
So the radius of convergence is actually equal to 1.
\end{example}

When the rational function is more complicated, it is also possible
to use method of partial fractions.  For example,
to find the Taylor series for $\frac{x^3+x}{x^2-1}$, we write
\begin{equation*}
\frac{x^3+x}{x^2-1}
=
x + \frac{1}{1+x} - \frac{1}{1-x}
=
x + \sum_{k=0}^\infty {(-1)}^k x^k - \sum_{k=0}^\infty x^k
=
- x + \sum_{\substack{k=3 \\ k \text{ odd}}}^\infty (-2) x^k .
\end{equation*}

\subsection{Exercises}

\begin{exercise}
Is the power series $\displaystyle \sum_{k=0}^\infty e^k x^k$ convergent?
If so, what is the radius of convergence?
\end{exercise}

\begin{exercise}
Is the power series $\displaystyle \sum_{k=0}^\infty k x^k$ convergent?
If so, what is the radius of convergence?
\end{exercise}

\begin{exercise}
Is the power series $\displaystyle \sum_{k=0}^\infty k! x^k$ convergent?
If so, what is the radius of convergence?
\end{exercise}

\begin{exercise}
Is the power series $\displaystyle \sum_{k=0}^\infty \frac{1}{(2k)!} {(x-10)}^k$
convergent?  If so, what is the radius of convergence?
\end{exercise}

\begin{exercise}
Determine the Taylor series for $\sin x$ around the point $x_0 = \pi$.
\end{exercise}

\begin{exercise}
Determine the Taylor series for $\ln x$ around the point $x_0 = 1$,
and find the radius of convergence.
\end{exercise}

\begin{exercise}
Determine the Taylor series
and its radius of convergence of $\dfrac{1}{1+x}$
around $x_0 = 0$.
\end{exercise}

\begin{exercise}
Determine the Taylor series and its radius of convergence
of
$\dfrac{x}{4-x^2}$ around $x_0 = 0$.  Hint: You will not be able to
use the ratio test.
\end{exercise}

\begin{exercise}
Expand $x^5+5x+1$ as a power series around $x_0 = 5$.
\end{exercise}

\begin{exercise}
Suppose that the ratio test applies to a series
$\displaystyle \sum_{k=0}^\infty a_k x^k$.  Show, using the ratio
test, that the radius of convergence of the differentiated
series is the same as that of the original series.
\end{exercise}

\begin{exercise}
Suppose that $f$ is an analytic function such that
$f^{(n)}(0) = n$.  Find $f(1)$.
\end{exercise}

\setcounter{exercise}{100}

\begin{exercise}
Is the power series
$\displaystyle \sum_{n=1}^\infty {(0.1)}^n x^n$
convergent? If so, what is the radius of convergence?
\end{exercise}
\exsol{%
Yes.  Radius of convergence is $10$.
}

\begin{exercise}[challenging]
Is the power series
$\displaystyle \sum_{n=1}^\infty \frac{n!}{n^n} x^n$
convergent? If so, what is the radius of convergence?
\end{exercise}
\exsol{%
Yes.  Radius of convergence is $e$.
}

\begin{exercise}
Using the geometric series, expand $\frac{1}{1-x}$ around $x_0=2$.
For what $x$ does the series converge?
\end{exercise}
\exsol{%
$\frac{1}{1-x} = -\frac{1}{1-(2-x)}$ so
$\frac{1}{1-x} =
\sum\limits_{n=0}^\infty {(-1)}^{n+1} {(x-2)}^n$,
which converges for $1 < x < 3$.
}

\begin{exercise}[challenging]
Find the Taylor series for $x^7 e^x$ around $x_0 = 0$.
\end{exercise}
\exsol{%
$\sum\limits_{n=7}^\infty
\frac{1}{(n-7)!} x^n$
}

\begin{exercise}[challenging]
Imagine $f$ and $g$ are analytic functions such that
$f^{(k)}(0) = g^{(k)}(0)$ for all large enough $k$.  What can you
say about $f(x)-g(x)$?
\end{exercise}
\exsol{%
$f(x)-g(x)$ is a polynomial.  Hint: Use Taylor series.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Series solutions of linear second-order ODEs}
\label{seriessols:section}

\sectionnotes{1.5 or 2 lectures\EPref{, \S8.2 in \cite{EP}}\BDref{,
\S5.2 and \S5.3 in \cite{BD}}}

Consider a linear second-order homogeneous ODE of the form
\begin{equation*}
P(x) y'' + Q(x) y' + R(x) y = 0 .
\end{equation*}
Suppose that $P(x)$, $Q(x)$, and $R(x)$ are polynomials.  We will 
try a solution of the form
\begin{equation*}
y = \sum_{k=0}^\infty a_k {(x-x_0)}^k
\end{equation*}
and solve for the $a_k$ to try to obtain a solution defined in some
interval around $x_0$.

The point $x_0$ is called an \emph{\myindex{ordinary point}}
if $P(x_0) \not= 0$.  That is, the functions
\begin{equation*}
\frac{Q(x)}{P(x)} \qquad \text{and} \qquad \frac{R(x)}{P(x)}
\end{equation*}
are defined for $x$ near $x_0$.  If $P(x_0) = 0$, then we say $x_0$
is a \emph{\myindex{singular point}}.  Handling singular points is
harder than ordinary points and so we now focus only on ordinary points.

\begin{example}
We start with a very simple example
\begin{equation*}
y'' - y = 0 .
\end{equation*}
Let us try a power series solution near $x_0 = 0$,
which is an ordinary point.  Every point is an ordinary
point in fact, as the equation is a constant-coefficient one.  We already know
we should obtain exponentials or the hyperbolic sine and cosine,
but let us pretend we do not know this fact.

We try
\begin{equation*}
y = \sum_{k=0}^\infty a_k x^k .
\end{equation*}
If we differentiate, the $k=0$ term is a constant and hence disappears.
We get
\begin{equation*}
y' = \sum_{k=1}^\infty k a_k x^{k-1} .
\end{equation*}
We differentiate yet again to obtain (now the $k=1$ term disappears)
\begin{equation*}
y'' = \sum_{k=2}^\infty k(k-1) a_k x^{k-2} .
\end{equation*}
We reindex the series (replace $k$ with $k+2$) to obtain
\begin{equation*}
y'' = \sum_{k=0}^\infty (k+2)\,(k+1) \, a_{k+2} x^k .
\end{equation*}
Now we plug $y$ and $y''$ into the differential equation
\begin{equation*}
\begin{split}
0 = y''-y & = 
\Biggl( \sum_{k=0}^\infty (k+2)\,(k+1) \, a_{k+2} x^k  \Biggr)
-
\Biggl( \sum_{k=0}^\infty a_k x^k \Biggr)
\\
& =
\sum_{k=0}^\infty \,\Bigl( (k+2)\,(k+1) \, a_{k+2} x^k 
-
a_k x^k \Bigr)
\\
& =
\sum_{k=0}^\infty \,\bigl( (k+2)\,(k+1) \,a_{k+2} - a_k \bigr) \, x^k  .
\end{split}
\end{equation*}
As $y'' - y$ is supposed to be equal to 0, we know that the
coefficients of the resulting series must be equal to 0.  Therefore,
\begin{equation*}
(k+2)\,(k+1) \,a_{k+2} - a_k = 0 ,
\qquad
\text{or}
\qquad
a_{k+2} = \frac{a_k}{(k+2)(k+1)} .
\end{equation*}
The equation above is called a \emph{\myindex{recurrence relation}}
for the coefficients of the power series.
It does not matter what $a_0$ or $a_1$ are.  They can be arbitrary.
But once we pick $a_0$ and $a_1$, all other coefficients are
determined by the recurrence relation.

Let us see what the coefficients
must be.  First, $a_0$ and $a_1$ are arbitrary.  Then,
\begin{equation*}
a_2 = \frac{a_0}{2}, \quad
a_3 = \frac{a_1}{(3)(2)}, \quad
a_4 = \frac{a_2}{(4)(3)} = \frac{a_0}{(4)(3)(2)}, \quad
a_5 = \frac{a_3}{(5)(4)} = \frac{a_1}{(5)(4)(3)(2)}, \quad \ldots
\end{equation*}
For even $k$, that is, $k=2n$,
we have
\begin{equation*}
a_k = a_{2n} = \frac{a_0}{(2n)!} .
\end{equation*}
For odd $k$, that is, $k=2n+1$, we have
\begin{equation*}
a_k = a_{2n+1} = \frac{a_1}{(2n+1)!} .
\end{equation*}
We write down the series for the solution
\begin{equation*}
y =
\sum_{k=0}^\infty
a_k x^k
=
\sum_{n=0}^\infty
\left(
\frac{a_0}{(2n)!} \,x^{2n}
+
\frac{a_1}{(2n+1)!} \,x^{2n+1}
\right)
=
a_0
\sum_{n=0}^\infty
\frac{1}{(2n)!} \,x^{2n}
+
a_1
\sum_{n=0}^\infty
\frac{1}{(2n+1)!} \,x^{2n+1} .
\end{equation*}
We recognize the two series as the hyperbolic sine and cosine.
Therefore,
\begin{equation*}
y =
a_0 \cosh x + a_1 \sinh x .
\end{equation*}
\end{example}

Of course, in general we will not be able to recognize 
the series that appears, since usually there will not be
any elementary function that matches it.  In that case, we will be
content with the series.

\begin{example}
Let us do a more complex example.  Consider
\emph{\myindex{Airy's equation}}%
\footnote{Named after the English mathematician
\href{http://en.wikipedia.org/wiki/George_Biddell_Airy}{Sir George Biddell Airy}
(1801--1892).}:
\begin{equation*}
y'' - xy = 0 ,
\end{equation*}
near the point $x_0 = 0$.  Note that $x_0 = 0$ is an ordinary point.

\pagebreak[2]
We try
\begin{equation*}
y = \sum_{k=0}^\infty a_k x^k .
\end{equation*}
We differentiate twice (as above) to obtain
\begin{equation*}
y'' = \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2} .
\end{equation*}
We plug $y$ into the equation
\begin{equation*}
\begin{split}
0 = y''-xy &= 
\Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2}  \Biggr)
-
x
\Biggl( \sum_{k=0}^\infty a_k x^k \Biggr)
\\
&=
\Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2}  \Biggr)
-
\Biggl( \sum_{k=0}^\infty a_k x^{k+1} \Biggr) .
\end{split}
\end{equation*}
We reindex to make things easier to sum
\begin{equation*}
\begin{split}
0 = y''-xy
&= 
\Biggl( 2 a_2 + \sum_{k=1}^\infty (k+2)\,(k+1) \, a_{k+2} x^k  \Biggr)
-
\Biggl( \sum_{k=1}^\infty a_{k-1} x^k \Biggr)
\\
&= 
2 a_2 + 
\sum_{k=1}^\infty \Bigl( (k+2)\,(k+1) \, a_{k+2} - a_{k-1} \Bigr) \, x^k .
\end{split}
\end{equation*}
Again $y''-xy$ is supposed to be 0, so $a_2 = 0$, and
\begin{equation*}
(k+2)\,(k+1) \,a_{k+2} - a_{k-1} = 0 ,
\qquad
\text{or}
\qquad
a_{k+2} = \frac{a_{k-1}}{(k+2)(k+1)} .
\end{equation*}
We jump in steps of three.  First, since $a_2 = 0$,
we must have $a_5 = 0$, $a_8 = 0$, $a_{11}=0$, etc.
In general, $a_{3n+2} = 0$.

The constants $a_0$ and $a_1$ are arbitrary and we obtain
\begin{equation*}
a_3 = \frac{a_0}{(3)(2)}, \quad
a_4 = \frac{a_1}{(4)(3)}, \quad
a_6 = \frac{a_3}{(6)(5)} = \frac{a_0}{(6)(5)(3)(2)}, \quad
a_7 = \frac{a_4}{(7)(6)} = \frac{a_1}{(7)(6)(4)(3)}, \quad \ldots
\end{equation*}
For $a_k$ where $k$ is a multiple of $3$, that is, $k=3n$, we notice
that
\begin{equation*}
a_{3n} = \frac{a_0}{(2)(3)(5)(6) \cdots (3n-1)(3n)} .
\end{equation*}
For $a_k$ where $k = 3n+1$, we notice
\begin{equation*}
a_{3n+1} = \frac{a_1}{(3)(4)(6)(7) \cdots (3n)(3n+1)} .
\end{equation*}
In other words, if we write down the series for $y$,
it has two parts
\begin{equation*}
\begin{split}
y &=
\left(
a_0 + \frac{a_0}{6} x^3 + \frac{a_0}{180} x^6 + \cdots +
\frac{a_0}{(2)(3)(5)(6) \cdots (3n-1)(3n)} x^{3n} + \cdots
\right)
\\
&\phantom{=}
+
\left(
a_1 x + \frac{a_1}{12} x^4 + \frac{a_1}{504} x^7 + \cdots +
\frac{a_1}{(3)(4)(6)(7) \cdots (3n)(3n+1)} x^{3n+1} + \cdots
\right)
\\
& =
a_0
\left(
1 + \frac{1}{6} x^3 + \frac{1}{180} x^6 + \cdots +
\frac{1}{(2)(3)(5)(6) \cdots (3n-1)(3n)} x^{3n} + \cdots
\right)
\\
&\phantom{=}
+
a_1
\left(
x + \frac{1}{12} x^4 + \frac{1}{504} x^7 + \cdots +
\frac{1}{(3)(4)(6)(7) \cdots (3n)(3n+1)} x^{3n+1} + \cdots
\right) .
\end{split}
\end{equation*}
We define
\begin{align*}
y_1(x) &= 
1 + \frac{1}{6} x^3 + \frac{1}{180} x^6 + \cdots +
\frac{1}{(2)(3)(5)(6) \cdots (3n-1)(3n)} x^{3n} + \cdots, \\
y_2(x) &= 
x + \frac{1}{12} x^4 + \frac{1}{504} x^7 + \cdots +
\frac{1}{(3)(4)(6)(7) \cdots (3n)(3n+1)} x^{3n+1} + \cdots ,
\end{align*}
and write the general solution to the equation as
$y(x)= a_0 y_1(x) + a_1 y_2(x)$.  If we plug in $x=0$ into the
power series for $y_1$ and $y_2$, we find
$y_1(0) = 1$ and $y_2(0) = 0$.  Similarly,
$y_1'(0) = 0$ and $y_2'(0) = 1$.  Therefore $y = a_0 y_1 + a_1 y_2$
is a solution
that satisfies the initial conditions $y(0) = a_0$ and $y'(0) = a_1$.

\begin{myfig}
\capstart
\diffyincludegraphics{width=3in}{width=4.5in}{ps-airy}
\caption{The two solutions $y_1$ and $y_2$ to Airy's equation.\label{ps:airyfig}}
\end{myfig}
\end{example}
The functions $y_1$ and $y_2$ cannot be written in terms of the elementary
functions that you know.  See \figurevref{ps:airyfig} for the plot of
the solutions $y_1$ and $y_2$.  These functions have many interesting
properties.  For example, they are oscillatory for negative $x$
(like solutions to $y''+y=0$) and
for positive $x$ they grow without bound (like solutions to $y''-y=0$).

\medskip

Sometimes a series solution may turn out to be a polynomial.
Let us see an example.

\begin{example}
Let us find a solution to the so-called
\emph{\myindex{Hermite's equation of order $n$}}%
\footnote{Named after the French mathematician
\href{http://en.wikipedia.org/wiki/Hermite}{Charles Hermite}
(1822--1901).}:
\begin{equation*}
y'' -2xy' + 2n y = 0 .
\end{equation*}

Let us find a solution around the point $x_0 = 0$.
We try
\begin{equation*}
y = \sum_{k=0}^\infty a_k x^k .
\end{equation*}
We differentiate (as above) to obtain
\begin{align*}
y' &= \sum_{k=1}^\infty k a_k x^{k-1} ,
\\
y'' &= \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2} .
\end{align*}

Now we plug into the equation
\begin{equation*}
\begin{split}
0 &= y''-2xy'+2ny \\
 &= 
\Biggl( \sum_{k=2}^\infty k(k-1) a_k x^{k-2}  \Biggr)
-
2x
\Biggl( \sum_{k=1}^\infty k a_k x^{k-1} \Biggr)
+
2n
\Biggl( \sum_{k=0}^\infty a_k x^k \Biggr)
\\
&=
\Biggl( \sum_{k=2}^\infty k(k-1) a_k x^{k-2}  \Biggr)
-
\Biggl( \sum_{k=1}^\infty 2k a_k x^k \Biggr)
+
\Biggl( \sum_{k=0}^\infty 2n a_k x^k \Biggr)
\\
&=
\Biggl(2a_2+
 \sum_{k=1}^\infty (k+2)(k+1) a_{k+2} x^k  \Biggr)
-
\Biggl( \sum_{k=1}^\infty 2k a_k x^k \Biggr)
+
\Biggl(
2na_0 + 
\sum_{k=1}^\infty 2n a_k x^k \Biggr)
\\
&=
2a_2+2na_0+
\sum_{k=1}^\infty \bigl( (k+2)(k+1)  a_{k+2} - 2ka_k + 2n a_k \bigr) x^k .
\end{split}
\end{equation*}
As $y''-2xy'+2ny = 0$, we have
\begin{equation*}
(k+2)(k+1)  a_{k+2} + ( - 2k+ 2n) a_k = 0 ,
\qquad
\text{or}
\qquad
a_{k+2} = \frac{(2k-2n)}{(k+2)(k+1)} a_k .
\end{equation*}
This recurrence relation actually includes
$a_2 = -na_0$ (which comes about from the constant term $2a_2+2na_0 = 0$).
Again $a_0$ and $a_1$ are arbitrary.
\begin{align*}
& a_2 = \frac{-2n}{(2)(1)}a_0, \qquad
a_3 = \frac{2(1-n)}{(3)(2)} a_1,
\displaybreak[0]\\
& a_4 = \frac{2(2-n)}{(4)(3)} a_2 = \frac{2^2(2-n)(-n)}{(4)(3)(2)(1)} a_0 ,
\displaybreak[0]\\
&
a_5 = \frac{2(3-n)}{(5)(4)} a_3 = \frac{2^2(3-n)(1-n)}{(5)(4)(3)(2)} a_1 ,
\quad \ldots
\end{align*}
We separate the even and odd coefficients to find that
\begin{align*}
a_{2m} &=\frac{2^m(-n)(2-n)\cdots(2m-2-n)}{(2m)!} , \\
a_{2m+1} &=\frac{2^m(1-n)(3-n)\cdots(2m-1-n)}{(2m+1)!} .
\end{align*}
We write down the two series, one with the even powers and one with the
odd.
\begin{align*}
y_1(x) & = 
1+\frac{2(-n)}{2!} x^2 + \frac{2^2(-n)(2-n)}{4!} x^4 + 
\frac{2^3(-n)(2-n)(4-n)}{6!} x^6 + \cdots ,
\\
y_2(x) & = 
x+\frac{2(1-n)}{3!} x^3 + \frac{2^2(1-n)(3-n)}{5!} x^5 + 
\frac{2^3(1-n)(3-n)(5-n)}{7!} x^7 + \cdots .
\end{align*}
Then
\begin{equation*}
y(x) = a_0 y_1(x) + a_1 y_2(x) .
\end{equation*}

We remark that if $n$ is a positive even integer, then $y_1(x)$ is a
polynomial as all the coefficients in the series beyond
degree $n$ are zero.  If $n$ is a positive odd integer, then $y_2(x)$ is
a polynomial.  For example, if $n=4$, then
\begin{equation*}
y_1(x) = 1 + \frac{2(-4)}{2!} x^2 + \frac{2^2(-4)(2-4)}{4!} x^4
= 1 - 4x^2 + \frac{4}{3} x^4 .
\end{equation*}
\end{example}

\subsection{Exercises}

In the following exercises, when asked to solve an equation using power
series methods, you should find the first few terms of the series,
and if possible find a general formula for the $k^{\text{th}}$ coefficient.

\begin{exercise}
Use power series methods to solve $y''+y = 0$ at the point $x_0 = 1$.
\end{exercise}

\begin{exercise}
Use power series methods to solve $y''+4xy = 0$ at the point $x_0 = 0$.
\end{exercise}

\begin{exercise}
Use power series methods to solve $y''-xy = 0$ at the point $x_0 = 1$.
\end{exercise}

\begin{exercise}
Use power series methods to solve $y''+x^2y = 0$ at the point $x_0 = 0$.
\end{exercise}

\begin{exercise}
The methods work for other orders than second order.  Try the methods
of this section to solve the first-order system $y'-xy = 0$ at
the point $x_0 = 0$.
\end{exercise}

\begin{exercise}[\myindex{Chebyshev's equation of order $p$}]
\leavevmode
\begin{tasks}
\task Solve $(1-x^2)y''-xy' + p^2y = 0$ using power series methods at $x_0=0$.
\task For what $p$ is there a polynomial solution?
\end{tasks}
\end{exercise}

\begin{exercise}
Find a polynomial solution to $(x^2+1) y''-2xy'+2y = 0$ using
power series methods.
\end{exercise}

\begin{exercise}
\leavevmode
\begin{tasks}
\task Use power series methods to solve $(1-x)y''+y = 0$ at the point $x_0 = 0$.
\task Use the solution to part a) to find a solution
for $xy''+y=0$ around the point $x_0=1$.
\end{tasks}
\end{exercise}

\setcounter{exercise}{100}

\begin{exercise}
Use power series methods to solve $y'' + 2 x^3 y = 0$ at the point $x_0 =
0$.
\end{exercise}
\exsol{%
%\begin{equation*}
%\begin{split}
%0 = y''+2 x^3 y &= 
%\Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2}  \Biggr)
%+
%2 x^3
%\Biggl( \sum_{k=0}^\infty a_k x^k \Biggr)
%\\
%&=
%\Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2}  \Biggr)
%+
%\Biggl( \sum_{k=0}^\infty 2 a_k x^{k+3} \Biggr) .
%\\
%&=
%\Biggl( \sum_{k=0}^\infty (k+2)\,(k+1) \, a_{k+2} x^k  \Biggr)
%+
%\Biggl( \sum_{k=3}^\infty 2 a_{k-3} x^k \Biggr) .
%\\
%&=
%2 a_2 +
%6 a_3 x +
%12 a_4 x^2 +
%\Biggl( \sum_{k=3}^\infty (k+2)\,(k+1) \, a_{k+2} x^k  \Biggr)
%+
%\Biggl( \sum_{k=3}^\infty 2 a_{k-3} x^k \Biggr) .
%\end{split}
%\end{equation*}
$a_2 = 0$, $a_3 = 0$, $a_4 = 0$, recurrence relation (for $k \geq 5$): $a_k
= \frac{- 2 a_{k-5}}{k(k-1)}$,
so:\\
$y(x) = a_0 + a_1 x -\frac{a_0}{10} x^5 - \frac{a_1}{15} x^6
+ \frac{a_0}{450} x^{10} + \frac{a_1}{825} x^{11}
- \frac{a_0}{47250} x^{15} - \frac{a_1}{99000} x^{16}
+
\cdots$
}

\pagebreak[2]
\begin{exercise}[challenging]
Power-series methods also work for nonhomogeneous equations.
\begin{tasks}
\task Use power series methods to solve $y'' - x y = \frac{1}{1-x}$
at the point $x_0 = 0$. Hint: Recall the geometric series.
\task Now solve for the initial condition $y(0)=0$, $y'(0)=0$.
\end{tasks}
\end{exercise}
\exsol{%
%\begin{equation*}
%\begin{split}
%\frac{1}{1-x} =
%\sum_{k=0}^\infty x^k
%=
%y''-x y &= 
%\Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2}  \Biggr)
%-
%x
%\Biggl( \sum_{k=0}^\infty a_k x^k \Biggr)
%\\
%&=
%\Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2}  \Biggr)
%-
%\Biggl( \sum_{k=0}^\infty a_k x^{k+1} \Biggr) .
%\\
%&=
%\Biggl( \sum_{k=0}^\infty (k+2)\,(k+1) \, a_{k+2} x^k  \Biggr)
%-
%\Biggl( \sum_{k=1}^\infty a_{k-1} x^k \Biggr) .
%\\
%&=
%2 a_2 + 
%\Biggl( \sum_{k=1}^\infty (k+2)\,(k+1) \, a_{k+2} x^k  \Biggr)
%-
%\Biggl( \sum_{k=1}^\infty a_{k-1} x^k \Biggr) .
%\end{split}
%\end{equation*}
a) $a_2 = \frac{1}{2}$, and for $k \geq 3$ we have
$a_k = \frac{a_{k-3} + 1}{k(k-1)}$, so \\
$y(x) = a_0 + a_1 x + \frac{1}{2} x^2
+ \frac{a_0 + 1}{6} x^3
+ \frac{a_1 + 1}{12} x^4
+ \frac{3}{40} x^5
+ \frac{a_0 + 7}{180} x^6
+ \frac{a_1 + 13}{504} x^7
+ \frac{43}{2240} x^8
+ \frac{a_0 + 187}{12960} x^9
+ \frac{a_1 + 517}{45360} x^{10} +
\cdots$
\\
b)
$y(x) = \frac{1}{2} x^2
+ \frac{1}{6} x^3
+ \frac{1}{12} x^4
+ \frac{3}{40} x^5
+ \frac{7}{180} x^6
+ \frac{13}{504} x^7
+ \frac{43}{2240} x^8
+ \frac{187}{12960} x^9
+ \frac{517}{45360} x^{10} +
\cdots$
}

\begin{exercise}
Attempt to solve $x^2 y'' - y = 0$ at $x_0 = 0$ using the power series
method of this section ($x_0$ is a singular point).
Can you find at least one solution?  Can you find more than one solution?
\end{exercise}
\exsol{%
%\begin{equation*}
%\begin{split}
%0 = x^2 y''-y &= 
%x^2 \Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^{k-2}  \Biggr)
%-
%\Biggl( \sum_{k=0}^\infty a_k x^k \Biggr)
%\\
%&=
%\Biggl( \sum_{k=2}^\infty k\,(k-1) \, a_k x^k  \Biggr)
%-
%a_0
%-
%a_1 x
%-
%\Biggl( \sum_{k=2}^\infty a_k x^k \Biggr) .
%\end{split}
%\end{equation*}
%so $a_0 = 0$, $a_1 = 0$, $k(k-1) a_k = a_k$
Applying the method of this section directly we obtain $a_k = 0$ for
all $k$ and so $y(x) = 0$ is the only solution we find.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Singular points and the method of Frobenius}
\label{frobenius:section}

%mbxINTROSUBSECTION

\sectionnotes{1.5 or 2 lectures\EPref{, \S8.4 and \S8.5 in \cite{EP}}\BDref{,
\S5.4--\S5.7 in \cite{BD}}}

The behavior of ODEs at singular points can be complicated.
For certain singular points,
we can find a solution on at least one side of the singular point
using a modification of
the power series.
Let us look at some examples before giving a general method.
%We may be lucky and obtain a power series
%solution using the method of the previous section, but in general we
%have to try other things.

\subsection{Examples}

\begin{example}
Consider the simple first-order equation 
\begin{equation*}
2 x y' - y = 0 .
\end{equation*}
Note that $x=0$ is a singular point.
Setting $x=0$ in the equation, we find
that any solution defined near zero satisfies
$y(0)=0$, but it is even worse.
If we try to plug in
\begin{equation*}
y = \sum_{k=0}^\infty a_k x^k ,
\end{equation*}
we obtain
\begin{equation*}
\begin{split}
0 = 2 xy'-y &= 
2x \, \Biggl( \sum_{k=1}^\infty k a_k x^{k-1}  \Biggr)
-
\Biggl( \sum_{k=0}^\infty a_k x^k \Biggr)
\\
& =
-a_0 + 
\sum_{k=1}^\infty (2 k a_k - a_k) \, x^{k} .
\end{split}
\end{equation*}
First, $a_0 = 0$.  Next, the only way to solve
$0 = 2 k a_k - a_k = (2k-1) \, a_k$
for $k = 1,2,3,\dots$ is for $a_k = 0$ for all $k$.
Therefore, in this manner we only get the trivial solution $y=0$.  We need
a nonzero solution to get the general solution to the equation.

Let us try $y=x^r$
for some real number $r$.  
Consequently our solution---if we can
find one---may only make sense for positive $x$.
Then $y' = r x^{r-1}$.  So
\begin{equation*}
0 = 2 x y' - y = 2 x r x^{r-1} - x^r = (2r-1) x^r .
\end{equation*}
Thus $r= \nicefrac{1}{2}$ and so $y = x^{1/2}$.
As the equation is linear,
the general solution for positive $x$ is
\begin{equation*}
y = C x^{1/2} .
\end{equation*}
If $C \not= 0$, then
the derivative of the solution \myquote{blows up} at $x=0$ (the
singular point).  There is only one solution that is differentiable
at $x=0$ and that's the trivial solution $y=0$.
\end{example}

Not every problem with a singular point has a solution of the form $y=x^r$, of
course.  But perhaps we can combine the methods.  What we will do is
to try a solution of the form
\begin{equation*}
y = x^r f(x)
\avoidbreak
\end{equation*}
for positive $x$,
where $f(x)$ is an analytic function (a power series).

\begin{example}
Consider the equation
\begin{equation*}
4 x^2 y'' - 4 x^2 y' + (1-2x)y = 0,
\end{equation*}
and again note that $x=0$ is a singular point.

Let us try
\begin{equation*}
y = x^r \sum_{k=0}^\infty a_k x^k
= \sum_{k=0}^\infty a_k x^{k+r} ,
\end{equation*}
where $r$ is a real number, not necessarily an integer.
Again if such a solution exists, it may only exist for positive $x$.
First we find the derivatives
\begin{align*}
y' & = \sum_{k=0}^\infty (k+r)\, a_k x^{k+r-1} , \\
y'' & = \sum_{k=0}^\infty (k+r)\,(k+r-1)\, a_k x^{k+r-2} .
\end{align*}
We plug those into our equation:
\begin{equation*}
\begin{split}
0 & = 4x^2y''-4x^2y'+(1-2x)y
\\
&= 
4x^2 \, \Biggl( \sum_{k=0}^\infty (k+r)\,(k+r-1) \, a_k x^{k+r-2}  \Biggr)
-
4x^2 \, \Biggl( \sum_{k=0}^\infty (k+r) \, a_k x^{k+r-1}  \Biggr)
+
(1-2x)
\Biggl( \sum_{k=0}^\infty a_k x^{k+r} \Biggr)
\\
&=
\Biggl( \sum_{k=0}^\infty 4 (k+r)\,(k+r-1) \, a_k x^{k+r}  \Biggr)
\\
& \phantom{mmm}
-
\Biggl( \sum_{k=0}^\infty 4 (k+r) \, a_k x^{k+r+1}  \Biggr)
+
\Biggl( \sum_{k=0}^\infty a_k x^{k+r} \Biggr)
-
\Biggl( \sum_{k=0}^\infty 2a_k x^{k+r+1} \Biggr)
\\
&=
\Biggl( \sum_{k=0}^\infty 4 (k+r)\,(k+r-1) \, a_k x^{k+r}  \Biggr)
\\
& \phantom{mmm}
-
\Biggl( \sum_{k=1}^\infty 4 (k+r-1) \, a_{k-1} x^{k+r}  \Biggr)
+
\Biggl( \sum_{k=0}^\infty a_k x^{k+r} \Biggr)
-
\Biggl( \sum_{k=1}^\infty 2a_{k-1} x^{k+r} \Biggr)
\\
&=
4r(r-1) \, a_0 x^r  + a_0 x^r + 
\sum_{k=1}^\infty
\Bigl( 4 (k+r)\,(k+r-1) \, a_k
-
4 (k+r-1) \, a_{k-1}
+
a_k
-
2a_{k-1} \Bigr) \, x^{k+r} 
\\
&=
\bigl( 4r(r-1) + 1 \bigr) \, a_0 x^r + 
\sum_{k=1}^\infty
\Bigl( \bigl( 4 (k+r)\,(k+r-1) + 1 \bigr) \, a_k
-
\bigl( 4 (k+r-1) + 2 \bigr) \, a_{k-1} \Bigr) \, x^{k+r} .
\end{split}
\end{equation*}
First, to have a solution we must have
$\bigl( 4r(r-1) + 1 \bigr) \, a_0 = 0$.  Supposing $a_0 \not= 0$,
\begin{equation*}
4r(r-1) + 1 = 0 .
\end{equation*}
This equation is called the \emph{\myindex{indicial equation}}.
This particular indicial
equation has a double root at $r = \nicefrac{1}{2}$.

OK\@, so we know what $r$ has to be.  That knowledge we obtained simply by looking
at the coefficient of $x^r$.  All other
coefficients of $x^{k+r}$ also have to be zero so
\begin{equation*}
\bigl( 4 (k+r)\,(k+r-1) + 1 \bigr) \, a_k
-
\bigl( 4 (k+r-1) + 2 \bigr) \, a_{k-1} = 0 .
\end{equation*}
If we plug in $r=\nicefrac{1}{2}$ and solve for $a_k$, we get
\begin{equation*}
a_k
=
\frac{4 (k+\nicefrac{1}{2}-1) + 2}{4 (k+\nicefrac{1}{2})\,(k+\nicefrac{1}{2}-1) + 1} \, a_{k-1}
=
\frac{1}{k} \, a_{k-1} .
\end{equation*}
Let us set $a_0 = 1$.  Then
\begin{equation*}
a_1 = \frac{1}{1} a_0 = 1 ,
\qquad
a_2 = \frac{1}{2} a_1 = \frac{1}{2} ,
\qquad
a_3 = \frac{1}{3} a_2 = \frac{1}{3 \cdot 2} ,
\qquad
a_4 = \frac{1}{4} a_3 = \frac{1}{4 \cdot 3 \cdot 2} ,
\qquad \cdots
\end{equation*}
Extrapolating, we notice that
\begin{equation*}
a_k = \frac{1}{k(k-1)(k-2) \cdots 3 \cdot 2} = \frac{1}{k!} .
\end{equation*}
In other words,
\begin{equation*}
y = 
\sum_{k=0}^\infty a_k x^{k+r}
=
\sum_{k=0}^\infty \frac{1}{k!} x^{k+1/2}
=
x^{1/2}
\sum_{k=0}^\infty \frac{1}{k!} x^{k}
=
x^{1/2}
e^x .
\end{equation*}
That was lucky!  In general, we will not be able to write the series in
terms of elementary functions.

We have one solution, let us call it $y_1 = x^{1/2} e^x$.
But what about a second solution?  If
we want a general solution, we need two linearly independent solutions.
Picking $a_0$ to be a different constant only gets us a constant
multiple of $y_1$, and we do not have any other $r$ to try; we only
have one solution to the indicial equation.  Well, there are powers of $x$
floating around and we are taking derivatives, perhaps the logarithm (the
antiderivative of $x^{-1}$) is around as well.  It turns out we want to
try for another solution of the form
\begin{equation*}
y_2 = \sum_{k=0}^\infty b_k x^{k+r} + (\ln x) y_1 ,
\end{equation*}
which in our case is
\begin{equation*}
y_2 = \sum_{k=0}^\infty b_k x^{k+1/2} + (\ln x) x^{1/2} e^x .
\end{equation*}
We now differentiate this equation, substitute into the differential
equation and solve for $b_k$.  A long computation ensues and we
obtain some recursion relation for $b_k$.  The reader
can (and should) try this to obtain for example the first three terms
\begin{equation*}
b_1 = b_0 -1 , \qquad b_2 = \frac{2b_1-1}{4} , \qquad b_3 =
\frac{6b_2-1}{18} , \qquad \ldots
\end{equation*}
We then fix $b_0$ and obtain a solution $y_2$.  Then
we write the general solution as $y = A y_1 + B y_2$.
\end{example}

\subsection{The method of Frobenius}

Before giving the general method, let us clarify when the method applies.
Let
\begin{equation*}
P(x) y'' + Q(x) y' + R(x) y = 0
\end{equation*}
be an ODE\@.  As before, if $P(x_0) = 0$, then $x_0$ is a
singular point.
If we divide by $P(x)$ to put the equation in standard form
$y'' + \frac{Q(x)}{P(x)} y' + \frac{R(x)}{P(x)} y = 0$,
perhaps the singularities introduced are not too bad.
More specifically, if the limits
\begin{equation*}
\lim_{x \to x_0} ~ (x-x_0) \frac{Q(x)}{P(x)}
\qquad \text{and} \qquad
\lim_{x \to x_0} ~ (x-x_0)^2 \frac{R(x)}{P(x)}
\end{equation*}
both exist and are finite, then we say that $x_0$ is
a \emph{\myindex{regular singular point}}.

\begin{example}
Often, and for the rest of this section, $x_0 = 0$.  Consider
\begin{equation*}
x^2y'' + x(1+x)y' + (\pi+x^2)y = 0 .
\end{equation*}
Write
\begin{align*}
& \lim_{x \to 0} ~x \frac{Q(x)}{P(x)} = 
\lim_{x \to 0} ~x \frac{x(1+x)}{x^2} = \lim_{x \to 0} ~(1+x) = 1 ,
\\
& \lim_{x \to 0} ~x^2 \frac{R(x)}{P(x)} = 
\lim_{x \to 0} ~x^2 \frac{(\pi+x^2)}{x^2} = \lim_{x \to 0} ~(\pi+x^2) = \pi .
\end{align*}
So $x = 0$ is a regular singular point.

On the other hand, if we make the slight change
\begin{equation*}
x^2y'' + (1+x)y' + (\pi+x^2)y = 0 ,
\end{equation*}
then
\begin{equation*}
\lim_{x \to 0} ~x \frac{Q(x)}{P(x)} = 
\lim_{x \to 0} ~x \frac{(1+x)}{x^2} = \lim_{x \to 0} ~\frac{1+x}{x} =
\text{DNE}.
\end{equation*}
Here DNE stands for \emph{does not exist}.
The point $0$ is singular, but not a regular singular point.
\end{example}

We now discuss the general \emph{\myindex{Method of Frobenius}}\index{Frobenius method}%
\footnote{Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Ferdinand_Georg_Frobenius}{Ferdinand
Georg Frobenius} (1849--1917).}.
We only consider the method at the point $x=0$ for simplicity.
If $x_0 \not=0$, then in the solution, we would replace every $x$ with $(x-x_0)$.
The main
idea is the following theorem.

\begin{theorem}[Method of Frobenius]
Suppose that 
\begin{equation} \label{eq:frobeniusmethod}
P(x) y'' + Q(x) y' + R(x) y = 0
\end{equation}
has a regular singular point at $x=0$, then there exists at least
one solution of the form
\begin{equation*}
y = x^r \sum_{k=0}^\infty a_k x^k .
\end{equation*}
A solution of this form is called a
\emph{\myindex{Frobenius-type solution}}.
\end{theorem}

\pagebreak[2]
The method usually breaks down like this:

\begin{enumerate}[(i)]
\item
We seek a Frobenius-type solution of the form
\begin{equation*}
y = \sum_{k=0}^\infty a_k x^{k+r} .
\end{equation*}
We plug this $y$ into equation \eqref{eq:frobeniusmethod}.  We collect
terms and write everything as a single series.
\item
The obtained series must be zero.  Setting the first
coefficient (usually the coefficient of $x^r$) in the series
to zero we obtain the
\emph{indicial equation}, which is a quadratic polynomial in $r$.
\item
If the indicial equation has two real roots $r_1$ and $r_2$
such that $r_1 - r_2$ is not an integer, then we have two linearly
independent Frobenius-type solutions.  Using the first root, we plug in
\begin{equation*}
y_1 = x^{r_1} \sum_{k=0}^\infty a_k x^{k} ,
\end{equation*}
and we solve for all $a_k$ to obtain the first solution.  Then
using the second root,
we plug in
\begin{equation*}
y_2 = x^{r_2} \sum_{k=0}^\infty b_k x^{k} ,
\end{equation*}
and solve for all $b_k$ to obtain the second solution.
\item
If the indicial equation has a doubled root $r$, then there we find 
one solution
\begin{equation*}
y_1 = x^{r} \sum_{k=0}^\infty a_k x^{k} ,
\end{equation*}
and then we obtain a new solution by plugging
\begin{equation*}
y_2 = x^{r} \sum_{k=0}^\infty b_k x^{k} + (\ln x) y_1 ,
\end{equation*}
into equation \eqref{eq:frobeniusmethod} and solving for the constants $b_k$.
\item
If the indicial equation has two real roots such that $r_1-r_2$ is
an integer, then one solution is
\begin{equation*}
y_1 = x^{r_1} \sum_{k=0}^\infty a_k x^{k} ,
\end{equation*}
and the second linearly independent solution is of the form
\begin{equation*}
y_2 = x^{r_2} \sum_{k=0}^\infty b_k x^{k} + C (\ln x) y_1 ,
\end{equation*}
where we plug $y_2$ into \eqref{eq:frobeniusmethod} and solve for the
constants $b_k$ and $C$.
\item
Finally, if the indicial equation has complex roots, then solving
for $a_k$ in the solution
\begin{equation*}
y = x^{r_1} \sum_{k=0}^\infty a_k x^{k}
\end{equation*}
results in a complex-valued function---all the $a_k$ are complex
numbers.  We obtain our two linearly independent
solutions%
\footnote{See 
Joseph L.\ Neuringera,
\emph{The Frobenius method for complex roots of the indicial equation},
International Journal of Mathematical Education in Science and Technology,
Volume 9, Issue 1, 1978, 71--77.}
by taking the real and imaginary parts of $y$.
\end{enumerate}

The main idea is to find at least one Frobenius-type solution.  If
we are lucky and find two, we are done.
If we only get one, we either use the ideas above or even a different method
such as reduction of order (see \sectionref{solinear:section}) to
obtain a second solution.

\subsection{Bessel functions} \label{bessel:subsection}

An important class of functions that arise commonly in physics are the
\emph{Bessel functions}%
\footnote{Named after
the German astronomer and mathematician
\href{http://en.wikipedia.org/wiki/Friedrich_Bessel}{Friedrich Wilhelm
Bessel} (1784--1846).}.
For example, these functions appear when solving the
wave equation in two and three dimensions.  First consider
\emph{\myindex{Bessel's equation}} of order $p$:
\begin{equation*}
x^2 y'' + xy' + \left(x^2 - p^2\right)y = 0 .
\end{equation*}
We allow $p$ to be any number, not just an integer, although integers
and multiples of $\nicefrac{1}{2}$ are most important in applications.

When we plug
\begin{equation*}
y = \sum_{k=0}^\infty a_k x^{k+r}
\end{equation*}
into Bessel's equation of order $p$, we obtain the indicial equation
\begin{equation*}
r(r-1)+r-p^2 = (r-p)(r+p) = 0 .
\end{equation*}
We obtain two roots, $r_1 = p$ and $r_2 = -p$.
If $p$ is not an integer, then following the method of Frobenius and
setting $a_0 = 1$, we find
linearly independent solutions of the form
\begin{align*}
& y_1 = x^p \sum_{k=0}^\infty
\frac{{(-1)}^k x^{2k}}{2^{2k} k! (k+p)(k-1+p)\cdots (2+p)(1+p)} ,
\\
& y_2 = x^{-p} \sum_{k=0}^\infty
\frac{{(-1)}^k x^{2k}}{2^{2k} k! (k-p)(k-1-p)\cdots (2-p)(1-p)} .
\end{align*}

\begin{exercise}
\leavevmode
\begin{tasks}
\task
Verify that the indicial equation of Bessel's equation of order $p$ is
$(r-p)(r+p)=0$.
\task
Suppose $p$ is not an integer.  Carry out the computation
to obtain the solutions $y_1$ and $y_2$ above.
\end{tasks}
\end{exercise}

Bessel functions are convenient constant multiples of $y_1$ and $y_2$.
First we must define the \emph{gamma function}
\begin{equation*}
\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} \, dt .
\end{equation*}
Notice that $\Gamma(1) = 1$.
The gamma function also has a wonderful property
\begin{equation*}
\Gamma(x+1) = x \Gamma(x) .
\end{equation*}
From this property, it follows that $\Gamma(n) = (n-1)!$ when $n$ is an
integer.  So the gamma function is a continuous version of the factorial.  We
compute:
\begin{align*}
& \Gamma(k+p+1)=(k+p)(k-1+p)\cdots (2+p)(1+p) \Gamma(1+p) ,
\\
& \Gamma(k-p+1)=(k-p)(k-1-p)\cdots (2-p)(1-p) \Gamma(1-p) .
\end{align*}

\begin{exercise}
Verify the identities above using 
$\Gamma(x+1) = x \Gamma(x)$.
\end{exercise}

We define the \emph{Bessel functions of the first kind}%
\index{Bessel function of the first kind} of
order $p$ and $-p$ as
\begin{align*}
& J_p(x) = \frac{1}{2^p\Gamma(1+p)} y_1
=
\sum_{k=0}^\infty
\frac{{(-1)}^k}{k! \, \Gamma(k+p+1)}
{\left(\frac{x}{2}\right)}^{2k+p} ,
\\
& J_{-p}(x) = \frac{1}{2^{-p}\Gamma(1-p)} y_2
=
\sum_{k=0}^\infty
\frac{{(-1)}^k}{k! \,\Gamma(k-p+1)}
{\left(\frac{x}{2}\right)}^{2k-p} .
\end{align*}
As these are constant multiples of the solutions we found above, these are
both solutions to Bessel's equation of order $p$.  The constants are picked
for convenience.

When $p$ is not an integer, $J_p$
and $J_{-p}$ are linearly independent.  When $n$ is an integer we 
obtain
\begin{equation*}
J_n(x) =
\sum_{k=0}^\infty
\frac{{(-1)}^k}{k! \,(k+n)!}
{\left(\frac{x}{2}\right)}^{2k+n} .
\end{equation*}
In this case
\begin{equation*}
J_n(x) = {(-1)}^nJ_{-n}(x) ,
\end{equation*}
and so $J_{-n}$ is not a second linearly independent
solution.  The other solution is the
so-called \emph{\myindex{Bessel function of second kind}}.  These make
sense only for integer orders $n$ and
are defined as limits of linear combinations of $J_p(x)$ and $J_{-p}(x)$, as
$p$ approaches $n$ in the
following way:
\begin{equation*}
Y_n(x) = \lim_{p\to n} \frac{\cos(p \pi) J_p(x) - J_{-p}(x)}{\sin(p \pi)} .
\end{equation*}
Each linear combination of $J_p(x)$ and $J_{-p}(x)$ is a solution
to Bessel's equation of order $p$.  Then as we take the limit as $p$
goes to $n$, we see that $Y_n(x)$ is a solution to Bessel's equation of
order $n$.  It also turns out that $Y_n(x)$ and $J_n(x)$ are linearly
independent.  Therefore when $n$ is an integer, we have the
general solution to Bessel's equation of order $n$:
\begin{equation*}
y = A J_n(x) + B Y_n(x) ,
\end{equation*}
for arbitrary constants $A$ and $B$.  Note that
$Y_n(x)$ goes to negative infinity at $x=0$.   Many mathematical software
packages have
these functions $J_n(x)$ and $Y_n(x)$ defined, so they can be used
just like say $\sin(x)$ and $\cos(x)$.  In fact, Bessel functions
have some similar 
properties.  For example, $-J_1(x)$ is a derivative of $J_0(x)$, and
in general the derivative of $J_n(x)$ can be written as a linear
combination of $J_{n-1}(x)$ and $J_{n+1}(x)$.  Furthermore, these
functions oscillate, although they are not periodic.  See
\figurevref{bessel:graphsfig} for graphs of Bessel functions.
\begin{myfig}
\capstart
%original files bessel-first bessel-second
\diffyincludegraphics{width=6.24in}{width=9in}{bessel-first-second}
\caption{Plot of the $J_0(x)$ and $J_1(x)$ in the first graph
and $Y_0(x)$ and $Y_1(x)$ in the second graph.\label{bessel:graphsfig}}
\end{myfig}

\begin{example}
Other equations can sometimes be solved in terms of the Bessel functions.
For example, given a positive constant $\lambda$,
\begin{equation*}
x y'' + y' + \lambda^2 x y = 0 ,
\end{equation*}
can be changed to 
$x^2 y'' + x y' + \lambda^2 x^2 y = 0$.  Changing variables
$t = \lambda x$, we obtain, via the chain rule, the equation in $y$ and $t$:
\begin{equation*}
t^2 y'' + t y' + t^2 y = 0 ,
\end{equation*}
which we recognize as Bessel's equation of order $0$.  Therefore the
general solution is $y(t) = A J_0(t) + B Y_0(t)$, or in terms of $x$:
\begin{equation*}
y = A J_0(\lambda x) + B Y_0(\lambda x) .
\end{equation*}
This equation comes up, for example, when finding the fundamental modes of
vibration of a circular drum, but we digress.
\end{example}

\subsection{Exercises}

\begin{exercise}
Find a particular (Frobenius-type) solution of $x^2 y'' + x y' + (1+x) y = 0$.
\end{exercise}

\begin{exercise}
Find a particular (Frobenius-type) solution of $x y'' - y = 0$.
\end{exercise}

\begin{exercise}
Find a particular (Frobenius-type) solution of $y'' +\frac{1}{x}y' - xy = 0$.
\end{exercise}

\begin{exercise}
Find the general solution of $2 x y'' + y' - x^2 y = 0$.
\end{exercise}

\begin{exercise}
Find the general solution of $x^2 y'' - x y' -y = 0$.
\end{exercise}

\begin{exercise}
In the following equations
classify the point $x=0$ as \emph{ordinary}, \emph{regular singular}, or
\emph{singular but not regular singular}.
\begin{tasks}(2)
\task $x^2(1+x^2)y''+xy=0$
\task $x^2y''+y'+y=0$
\task $xy''+x^3y'+y=0$
\task $xy''+xy'-e^xy=0$
\task $x^2y''+x^2y'+x^2y=0$
\end{tasks}
\end{exercise}

\setcounter{exercise}{100}

\begin{exercise}
In the following equations
classify the point $x=0$ as \emph{ordinary}, \emph{regular singular}, or
\emph{singular but not regular singular}.
\begin{tasks}(2)
\task $y''+y=0$
\task $x^3y''+(1+x)y=0$
\task $xy''+x^5y'+y=0$
\task $\sin(x)y''-y=0$
\task $\cos(x)y''-\sin(x)y=0$
\end{tasks}
\end{exercise}
\exsol{%
a)~ordinary, b)~singular but not regular singular, c)~regular singular,
d)~regular singular, e)~ordinary.
}

\begin{exercise}
Find the general solution of $x^2 y'' -y = 0$.
\end{exercise}
\exsol{%
$y = A x^{\frac{1+\sqrt{5}}{2}} + B x^{\frac{1-\sqrt{5}}{2}}$
}

\begin{exercise}
Find a particular solution of $x^2 y'' +(x-\nicefrac{3}{4})y = 0$.
\end{exercise}
\exsol{%
$y = x^{3/2}
\sum\limits_{k=0}^\infty
\frac{{(-1)}^{k}}{k!\,(k+2)!} x^k$ (Note that for convenience
we did not pick $a_0 = 1$.)}

\begin{exercise}[tricky]
Find the general solution of $x^2 y'' - x y' +y = 0$.
\end{exercise}
\exsol{%
$y = Ax + B x \ln(x)$
}

