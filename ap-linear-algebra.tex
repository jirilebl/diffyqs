\chapter{Linear algebra} \label{linalg:appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Vectors, mappings, and linear systems}
\label{vecsandmaps:section}

\sectionnotes{? lectures}

In real life, there is most often more than one variable.
So we wish to organize dealing with multiple variables in a consistent
manner, and in particular organize dealing with linear equations and linear
mappings, as those are rather easy to handle if done properly.
There is a joke running among some mathematicians:
\myquote{to an engineer every problem is linear, and everything is a
matrix.}
And well, they (the engineers) are not wrong. 
Quite often, solving an engineering problem really is figuring out the
right linear problem to solve.
Most importantly linear problems are the ones which we know how to solve
and we have lots and lots of tools to solve them.
So for engineers, mathematicians, physicists, or any other
scientists it is absolutely vital to learn linear algebra.

As motivation, suppose we have to solve
\begin{equation*}
\begin{aligned}
& x-y = 2 , \\
& 2x+y = 4 ,
\end{aligned}
\end{equation*}
for $x$ and $y$.
What you could do is add the equations together to find
\begin{equation*}
x+2x-y+y = 2+4, \qquad \text{or} \qquad 3x = 6 .
\end{equation*}
And now we find $x=3$.  Once we have that, we can plug in $x=2$ into the
first equation to find $2-y=2$, so $y=0$.  OK\@, that was easy.  What is all
this fuss about linear equations.  Well, try doing this if you have
5000 unknowns\footnote{One of the downsides of making everything look like a
linear problem is that the number of variables tends to become huge.}.
Also, we have such equations not of just numbers,
but of functions and derivatives of functions in differential equations.
Clearly we need a more systematic way of doing things.
A consequence of making things systematic and simpler to write down
is that it also becomes much easier to have computers do the work for us.
Computers are rather stupid, they do not think,
but are very good at doing lots of repetitive
tasks precisely, as long as we figure out a systematic way for them to
perform the tasks.

\medskip

Let us start with vectors.  Consider $n$ real numbers as an
$n$-tuple
\begin{equation*}
(x_1,x_2,\ldots,x_n). 
\end{equation*}
These points span the so-called
\emph{$n$-dimensional space}\index{n-dimensional space@$n$-dimensional space},
often denoted by ${\mathbb R}^n$.
Sometimes we call this the $n$-dimensional
\emph{\myindex{euclidean space}}\footnote{FIXME: Named after Euclid}.
For example in two dimensions, ${\mathbb R}^2$ is the
\emph{\myindex{cartesian plane}}\footnote{FIXME: Named after Descartes}.
So we have say
the point $(1,2)$, which is one unit to the right and two units up from the
origin.

When we do algebra with these $n$-tuples of numbers we call them
\emph{vectors}\index{vector}.  Mathematicians are keen on separating
what is a vector and what is a point of the space or in the plane,
and it turns out
to be an important distinction, however, for the purposes of linear algebra
we can think of everything being represented by a vector.
A way to think of a vector, which is especially useful in calculus
and differential equations, is an arrow.  It is an object that has
a direction and a magnitude.  For example, the vector $(1,2)$
is the arrow from the origin to the point $(1,2)$ in the plane.
See figure FIXME.

FIXME: figure.

For reasons that will become clear in the next section, it will often be
useful to write vectors as so-called
\emph{column vectors}\index{column vector}:
\begin{equation*}
\begin{bmatrix}
x_{1} \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} .
\end{equation*}
Don't worry, it is just a different way of writing the same thing, and
it will be useful later.  For example, the vector $(1,2)$ can be written as
\begin{equation*}
\begin{bmatrix}
1 \\ 2
\end{bmatrix} .
\end{equation*}

Now where does the algebra come in?  Well, arrows are easy to add, subtract
and multiply by numbers.  First adding.  If we have two arrows, we simply
move along one, and then along the other.  See Figure FIXME.

FIXME: figure.

It is rather easy to see what it does to the numbers that represent the
vectors.  Suppose I want to add $(1,2)$ to $(-1,3)$.  So I travel along $(1,2)$
and then travel along $(-1,3)$.
What I did was travel one unit right, two units up, and then
I travelled one unit left (the negative one), and three units up.  That
means that I added up at $(1-1,2+3) = (0,5)$.  And that's how addition
always works:
\begin{equation*}
\begin{bmatrix}
x_{1} \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} +
\begin{bmatrix}
y_{1} \\ y_2 \\ \vdots \\ y_n
\end{bmatrix} =
\begin{bmatrix}
x_1 + y_{1} \\ x_2+ y_2 \\ \vdots \\ x_n + y_n
\end{bmatrix} .
\end{equation*}

Another intuitive thing to do to a vector is to scale it.
We represent this by multiplication of a number with a vector.
Because of this, when we wish to distinguish between vectors and numbers, we
call the numbers \emph{scalars}\index{scalar}.
For example,
suppose we want to travel three times further.  If the vector is $(1,2)$,
travelling 3 times further means going 3 units to the right and 6 units up,
so we get the vector $(3,6)$.
That is, we just multiply each number in the vector by 3.  So if $\alpha$
is a number we say
\begin{equation*}
\alpha
\begin{bmatrix}
x_{1} \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} =
\begin{bmatrix}
\alpha x_{1} \\ \alpha x_2 \\ \vdots \\ \alpha x_n
\end{bmatrix} .
\end{equation*}
When the scalar is negative, then when we multiply a vector by it, the
vector is not only scaled, but it also changes orientation.  So for example
multiplying $(1,2)$ by $-3$ means we should go 3 times further but in the
opposite direction, so 3 units to the left and 6 units down, or in other
words, $(-3,-6)$.

Let's compute a small example:
\begin{equation*}
3
\begin{bmatrix}
1 \\ 2
\end{bmatrix}
+
2
\begin{bmatrix}
-4 \\ -1
\end{bmatrix} 
-
3
\begin{bmatrix}
-2 \\ 2
\end{bmatrix} 
=
\begin{bmatrix}
3(1)+2(-4)-3(-2) \\ 3(2)+2(-1)-3(2)
\end{bmatrix}
=
\begin{bmatrix}
1 \\ -2
\end{bmatrix}
.
\end{equation*}








FIXME: just a dump of stuff from chapter 3

A \emph{\myindex{matrix}}
is an $m
\times n$ array of numbers ($m$ rows and $n$ columns).  For example, we denote
a $3 \times 5$ matrix as follows
\begin{equation*}
A = 
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35}
\end{bmatrix} .
\end{equation*}
The numbers $a_{ij}$ are called \emph{elements}\index{element of a matrix}
or \emph{entries}\index{entry of a matrix}.

By a \emph{\myindex{vector}} we usually mean a \emph{\myindex{column
vector}}, that is an $m \times 1$ matrix.  If we mean a \emph{\myindex{row
vector}} we will explicitly say so (a row vector is a $1 \times n$ matrix).
We usually denote
matrices by upper case letters and vectors by lower case letters with an
arrow such as $\vec{x}$ or $\vec{b}$.  By $\vec{0}$ we mean the vector
of all zeros.

FIXME: define linear systems in terms of matrices, matrix multiplication
will be in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Matrix algebra}
\label{matalg:section}

\sectionnotes{? lectures}

FIXME  Below is just a dump of the section in chapter 3


We define some operations on matrices.  We 
want $1 \times 1$ matrices to really act like numbers, so our operations
have to be compatible with this viewpoint.

First, we can multiply\index{scalar multiplication} a matrix by
a \emph{\myindex{scalar}} (a number).
We simply multiply each entry in the matrix by the scalar.  For example,
\begin{equation*}
2
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix} =
\begin{bmatrix}
2 & 4 & 6 \\
8 & 10 & 12
\end{bmatrix} .
\end{equation*}
Matrix addition\index{addition of matrices} is also easy.
We add matrices element by element.
For example,
\begin{equation*}
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix} +
\begin{bmatrix}
1 & 1 & -1 \\
0 & 2 & 4
\end{bmatrix}
=
\begin{bmatrix}
2 & 3 & 2 \\
4 & 7 & 10
\end{bmatrix} .
\end{equation*}
If the sizes do not match, then addition is not defined.

If we denote by 0 the matrix with all zero entries, by
$c$, $d$ scalars, and by $A$, $B$, $C$ matrices, we
have the following familiar rules:
\begin{align*}
A + 0 & = A = 0 + A , \\
A + B & = B + A , \\
(A + B) + C & = A + (B + C) , \\
c(A+B) & = cA+cB, \\
(c+d)A & = cA + dA.
\end{align*}

Another useful operation for matrices is the so-called
\emph{\myindex{transpose}}.  This operation just swaps rows and columns of a
matrix.
The transpose of $A$ is denoted by $A^T$.  Example:
\begin{equation*}
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}^T =
\begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6 
\end{bmatrix}
\end{equation*}

\subsection{Matrix multiplication}

Let us now define matrix multiplication.  First we define the so-called
\emph{\myindex{dot product}} (or \emph{\myindex{inner product}}) of two vectors.
Usually this will be a row vector multiplied
with a column vector of the same size.  For the dot product we multiply
each pair of entries from the first and the second vector and we sum these
products.  The result is a single number.
For example,
\begin{equation*}
\begin{bmatrix}
a_1 & a_2 & a_3
\end{bmatrix}
\cdot
\begin{bmatrix}
b_1 \\
b_2 \\
b_3
\end{bmatrix}
= a_1 b_1 + a_2 b_2 + a_3 b_3 .
\end{equation*}
And similarly for larger (or smaller) vectors.

Armed with the dot product we define the
\emph{\myindex{product of matrices}}.
First let us denote by $\operatorname{row}_i(A)$ the $i^{\text{th}}$ row
of $A$ and by
$\operatorname{column}_j(A)$ the $j^{\text{th}}$ column of $A$.
For an $m \times n$ matrix $A$ and an $n \times p$ matrix $B$
we can define the product $AB$.  We let $AB$ be an $m \times p$
matrix whose $ij^{\text{th}}$ entry is the dot product
\begin{equation*}
\operatorname{row}_i(A) \cdot
\operatorname{column}_j(B) .
\end{equation*}
Do note how the sizes match up: $m \times n$ multiplied by $n \times p$ is 
$m \times p$.  Example:
\begin{multline*}
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
\begin{bmatrix}
1 & 0 & -1 \\
1 & 1 & 1 \\
1 & 0 & 0
\end{bmatrix}
= \\ =
\begin{bmatrix}
1\cdot 1 + 2\cdot 1 + 3 \cdot 1 &  &
1\cdot 0 + 2\cdot 1 + 3 \cdot 0 &  &
1\cdot (-1) + 2\cdot 1 + 3 \cdot 0 \\
4\cdot 1 + 5\cdot 1 + 6 \cdot 1 &  &
4\cdot 0 + 5\cdot 1 + 6 \cdot 0 &  &
4\cdot (-1) + 5\cdot 1 + 6 \cdot 0
\end{bmatrix}
=
\begin{bmatrix}
6 & 2 & 1 \\
15 & 5 & 1
\end{bmatrix}
\end{multline*}

\medskip

For multiplication we want an analogue of a 1.  This analogue is the
so-called \emph{\myindex{identity matrix}}.
The identity matrix is a square matrix with 1s on the
main diagonal and zeros everywhere else.  It is usually denoted by $I$.
For each size we have a different identity matrix and so sometimes we may denote
the size as a subscript.  For example, the $I_3$ would be the $3 \times 3$
identity matrix
\begin{equation*}
I = I_3 =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} .
\end{equation*}

We have the following rules for matrix multiplication.  Suppose that
$A$, $B$, $C$ are matrices of the correct sizes so that the following
make sense.  Let $\alpha$ denote a scalar (number).
\begin{align*}
A(BC) & = (AB)C, \\
A(B+C) & = AB + AC, \\
(B+C)A & = BA + CA, \\
\alpha(AB) & = (\alpha A)B = A(\alpha B), \\
IA & = A = AI .
\end{align*}

A few warnings are in order.
\begin{enumerate}[(i)]
\item $AB \not= BA$ in general (it may be true by fluke sometimes).  That is,
matrices do not \myindex{commute}.
For example, take
$A = \left[ \begin{smallmatrix} 1 & 1 \\ 1 & 1 \end{smallmatrix} \right]$
and
$B = \left[ \begin{smallmatrix} 1 & 0 \\ 0 & 2 \end{smallmatrix} \right]$.
\item $AB = AC$ does not necessarily imply $B=C$, even if $A$ is not 0.
\item $AB = 0$ does not necessarily mean that $A=0$ or $B=0$.
For example, take
$A = B = \left[ \begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix}
\right]$.
\end{enumerate}

For the last two items to hold we would need to \myquote{divide} by
a matrix.  This is where the \emph{\myindex{matrix inverse}} comes in.
Suppose that $A$ and $B$ are $n \times n$ matrices such that
\begin{equation*}
AB = I = BA .
\end{equation*}
Then we call $B$ the inverse of $A$ and we denote $B$ by $A^{-1}$.
If the inverse of $A$ exists, then we call $A$
\emph{invertible\index{invertible matrix}}.
If $A$ is not invertible we sometimes say $A$ is
\emph{singular\index{singular matrix}}.

If $A$ is invertible, then $AB = AC$ does imply
that $B = C$ (in particular the inverse of $A$ is unique).
We just multiply both sides by $A^{-1}$ (on the left) to get
$A^{-1}AB = A^{-1}AC$ or $IB=IC$ or $B=C$.
It is also not hard to see that ${(A^{-1})}^{-1} = A$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Elimination, rank, and inverse}
\label{elim:section}

\sectionnotes{? lectures}

FIXME: just a dump of stuff from chapter 3

One application of matrices we will need is to solve systems of
linear equations.  This is best shown by example.
Suppose that we have the following system of linear equations
\begin{align*}
          2 x_1 +           2 x_2 +           2 x_3 & = 2 , \\
\phantom{9} x_1 + \phantom{9} x_2 +           3 x_3 & = 5 , \\
\phantom{9} x_1 +           4 x_2 + \phantom{9} x_3 & = 10 .
\end{align*}

Without changing the solution,
we could swap equations in this system,
we could multiply any of the equations by a nonzero number, and
we could add a multiple of one equation to another equation.
It turns out these operations always suffice to find a solution.

It is easier to write the system as a matrix equation.
The system above can be
written as
\begin{equation*}
\begin{bmatrix}
2 & 2 & 2 \\
1 & 1 & 3 \\
1 & 4 & 1 
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix} 
=
\begin{bmatrix}
2 \\
5 \\
10
\end{bmatrix} .
\end{equation*}
To solve the system we put the coefficient matrix (the matrix on the left
hand side of the equation) together with the vector on the right and side
and get the
so-called
\emph{\myindex{augmented matrix}}
\begin{equation*}
\left[
\begin{array}{ccc|c}
2 & 2 & 2 & 2 \\
1 & 1 & 3 & 5 \\
1 & 4 & 1 & 10
\end{array}
\right] .
%\qquad
%\text{or just}
%\qquad
%\begin{bmatrix}
%2 & 2 & 2 & 2 \\
%1 & 1 & 3 & 5 \\
%1 & 4 & 1 & 10
%\end{bmatrix} .
\end{equation*}
We apply the following three elementary operations.
%\pagebreak[2]%
\begin{enumerate}[(i)]
\item Swap two rows.
\item Multiply a row by a nonzero number.
\item Add a multiple of one row to another row.
\end{enumerate}
We keep doing these operations until we get into a state where it is
easy to read off the answer, or until we get into a contradiction indicating
no solution, for example if we come up with an equation such as $0=1$.

Let us work through the example.  First multiply the first row by
$\nicefrac{1}{2}$ to obtain
\begin{equation*}
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
1 & 1 & 3 & 5 \\
1 & 4 & 1 & 10
\end{array}
\right] .
\end{equation*}
Now subtract the first row from the second and third row.
\begin{equation*}
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 0 & 2 & 4 \\
0 & 3 & 0 & 9
\end{array}
\right]
\end{equation*}
Multiply the last row by $\nicefrac{1}{3}$ and the second row by $\nicefrac{1}{2}$.
\begin{equation*}
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 0 & 1 & 2 \\
0 & 1 & 0 & 3
\end{array}
\right]
\end{equation*}
Swap rows 2 and 3.
\begin{equation*}
\left[
\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 0 & 3 \\
0 & 0 & 1 & 2
\end{array}
\right]
\end{equation*}
Subtract the last row from the first, then subtract the second row
from the first.
\begin{equation*}
\left[
\begin{array}{ccc|c}
1 & 0 & 0 & -4 \\
0 & 1 & 0 & 3 \\
0 & 0 & 1 & 2
\end{array}
\right]
\end{equation*}
If we think about what equations this augmented matrix represents, we see that
$x_1 = -4$, $x_2 = 3$, and $x_3 = 2$.  We try this solution in the original
system and, voil\`a, it works!

\begin{exercise}
Check that the solution above really solves the given equations.
\end{exercise}

We write this equation in matrix notation as
\begin{equation*}
A \vec{x} = \vec{b} ,
\end{equation*}
where $A$ is the matrix
$\left[ \begin{smallmatrix}
2 & 2 & 2 \\
1 & 1 & 3 \\
1 & 4 & 1 
\end{smallmatrix} \right]$ and $\vec{b}$ is the vector
$\left[ \begin{smallmatrix}
2 \\
5 \\
10
\end{smallmatrix} \right]$.  The solution can also be computed via the
inverse,
\begin{equation*}
\vec{x} = A^{-1} A \vec{x} = A^{-1} \vec{b} .
\end{equation*}

\medskip

It is
possible that the solution is not unique, or that no solution exists.
It is easy to tell if a solution does not exist.  If during the row
reduction you come up with a row where all the entries except the last one
are zero (the last entry in a row corresponds to the right hand side of the
equation) the system is \emph{inconsistent\index{inconsistent system}} and
has no solution.  For
example, for a system of 3 equations and 3 unknowns, if you find a row
such as $[\,0 \quad 0 \quad 0 ~\,|\,~ 1\,]$ in the augmented matrix,
you know the system is inconsistent.  That row corresponds to $0=1$.

\medskip

You generally try to use row operations until the following conditions
are satisfied.  The first nonzero entry in each row is called the
\emph{\myindex{leading entry}}.
\begin{enumerate}[(i)]
\item There is only one leading entry in each column.
\item All the entries above and below a leading entry are zero.
\item All leading entries are 1.
\end{enumerate}
Such a matrix is said to be in
\emph{\myindex{reduced row echelon form}}.  The variables
corresponding to columns with no leading entries are said to be
\emph{free variables\index{free variable}}.
Free variables mean that we can pick those variables
to be anything we want and then solve for the rest of the unknowns.

\begin{example}
The following augmented matrix is in reduced row echelon form.
\begin{equation*}
\left[
\begin{array}{ccc|c}
1 & 2 & 0 & 3 \\
0 & 0 & 1 & 1 \\
0 & 0 & 0 & 0
\end{array}
\right]
\end{equation*}
Suppose
the variables are $x_1$, $x_2$, and $x_3$.  Then $x_2$ is the
free variable, $x_1 = 3 - 2x_2$, and $x_3 = 1$.

\medskip

On the other hand if during the row reduction process you come up with the
matrix
\begin{equation*}
\left[
\begin{array}{ccc|c}
1 & 2 & 13 & 3 \\
0 & 0 & 1 & 1 \\
0 & 0 & 0 & 3
\end{array}
\right]
,
\end{equation*}
there is no need to go further.  The last row corresponds to
the equation $0 x_1 + 0 x_2 + 0 x_3 = 3$, which is preposterous.  Hence, no
solution exists.
\end{example}

\subsection{Computing the inverse}

If the matrix $A$ is square and there exists a unique solution
$\vec{x}$ to $A \vec{x} = \vec{b}$ for any $\vec{b}$ (there are no free
variables), then $A$ is invertible.
Multiplying both sides by $A^{-1}$, you can see that $\vec{x} =
A^{-1} \vec{b}$.  So it is useful to compute the inverse if you want to
solve the equation for many different right hand sides $\vec{b}$.

We have a formula for
the $2 \times 2$ inverse, but it is also not hard
to compute inverses of larger matrices.
While we will not have too much occasion to compute inverses for larger
matrices than $2 \times 2$ by hand, let us touch on how to do it.
Finding the inverse of $A$ is actually just solving a bunch of linear
equations.  If we can solve $A \vec{x}_k = \vec{e}_k$ where $\vec{e}_k$ is
the vector with all zeros except a 1 at the $k^{\text{th}}$ position, then
the inverse is the matrix with the columns $\vec{x}_k$ for $k=1,2,\ldots,n$
(exercise: why?).  Therefore, to find the inverse we write a larger $n
\times 2n$ augmented matrix $[ \,A ~|~ I\, ]$, where $I$ is the identity
matrix.
We then perform row reduction.
The reduced row echelon form of $[ \,A ~|~ I\, ]$ 
will be of the form $[ \,I ~|~ A^{-1}\, ]$ if and only if
$A$ is invertible.  We then just read off the inverse $A^{-1}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Determinant}
\label{det:section}

\sectionnotes{? lectures}

FIXME: below is just a dump of the stuff from chapter 3

For square matrices we define a useful quantity called the
\emph{\myindex{determinant}}.  We define
the determinant of a $1 \times 1$ matrix as the value of its only entry.
For a $2 \times 2$ matrix we define
\begin{equation*}
\det \left(
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\right)
\overset{\text{def}}{=}
ad-bc .
\end{equation*}

Before trying to define the
determinant for larger matrices, let us note
the meaning of the determinant.  Consider an $n \times n$ matrix
as a mapping of the $n$ dimensional euclidean space ${\mathbb{R}}^n$ to 
itself, where $\vec{x}$ gets sent to $A \vec{x}$.
In particular, a $2 \times 2$ matrix $A$ is a mapping of
the plane to itself.  The
determinant of 
$A$ is the factor by which the area of objects changes. 
If we take the unit square (square of side 1) in the plane, then
$A$ takes the square to a parallelogram of area $\lvert\det(A)\rvert$.  The sign
of $\det(A)$ denotes changing of orientation (negative if the axes get flipped).  For
example, let
\begin{equation*}
A =
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix} .
\end{equation*}
Then $\det(A) = 1+1 = 2$.  Let us see where the (unit) square with vertices
$(0,0)$, $(1,0)$, $(0,1)$, and $(1,1)$ gets sent.  Clearly $(0,0)$ gets sent
to $(0,0)$.  
\begin{equation*}
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix}
\begin{bmatrix}
1 \\ 0
\end{bmatrix} =
\begin{bmatrix}
1 \\
-1 
\end{bmatrix}
,
\qquad
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix}
\begin{bmatrix}
0 \\ 1
\end{bmatrix} =
\begin{bmatrix}
1 \\
1 
\end{bmatrix}
,
\qquad
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix}
\begin{bmatrix}
1 \\ 1
\end{bmatrix} =
\begin{bmatrix}
2 \\
0 
\end{bmatrix}
.
\end{equation*}
The image of the square is another square with vertices $(0,0)$, $(1,-1)$,
$(1,1)$, and $(2,0)$.  The
image square has
a side of length $\sqrt{2}$ and is therefore of area 2.

If you think back to high school geometry, you may have seen a formula for
computing the area of a \myindex{parallelogram}
with vertices $(0,0)$, $(a,c)$, $(b,d)$
and $(a+b,c+d)$.  And it is precisely
\begin{equation*}
\left\lvert \, \det \left(
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
\right) \, \right\lvert.
\end{equation*}
The vertical lines above mean absolute value.
The matrix $\left[ \begin{smallmatrix} a & b \\ c & d \end{smallmatrix}
\right]$
carries the unit square to the given parallelogram.

\medskip

Let us look at the determinant for larger matrices.  We define $A_{ij}$ as
the matrix $A$ with the $i^{\text{th}}$ row and the $j^{\text{th}}$ column
deleted.  To compute the determinant of a matrix, pick one row, say the
$i^{\text{th}}$ row and compute:
\begin{equation*}
\boxed{~~
\det (A) =
\sum_{j=1}^n
{(-1)}^{i+j}
a_{ij} \det (A_{ij}) .
~~}
\end{equation*}
For the first row we get
\begin{equation*}
\det (A) =
a_{11} \det (A_{11}) - 
a_{12} \det (A_{12}) + 
a_{13} \det (A_{13}) - 
\cdots
\begin{cases}
+ a_{1n} \det (A_{1n}) & \text{if } n \text{ is odd,} \\
- a_{1n} \det (A_{1n}) & \text{if } n \text{ even.}
\end{cases}
\end{equation*}
We alternately add and subtract the determinants of the submatrices
$A_{ij}$ multiplied by $a_{ij}$ for a fixed $i$ and all $j$.
For a $3 \times 3$ matrix,
picking the first row, we get $\det (A) = a_{11} \det (A_{11}) -
a_{12} \det (A_{12}) + a_{13} \det (A_{13})$.  For example,
\begin{equation*}
\begin{split}
\det \left(
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
\right)
& =
1 \cdot
\det \left(
\begin{bmatrix}
5 & 6 \\
8 & 9
\end{bmatrix}
\right)
-
2 \cdot
\det \left(
\begin{bmatrix}
4 & 6 \\
7 & 9
\end{bmatrix}
\right)
+
3 \cdot
\det \left(
\begin{bmatrix}
4 & 5 \\
7 & 8
\end{bmatrix}
\right) \\
& =
1 (5 \cdot 9 - 6 \cdot 8)
-
2 (4 \cdot 9 - 6 \cdot 7)
+
3 (4 \cdot 8 - 5 \cdot 7)
= 0 .
\end{split}
\end{equation*}

The numbers ${(-1)}^{i+j}\det(A_{ij})$ are called
\emph{cofactors\index{cofactor}}
of the matrix and
this way of computing the determinant is called the
\emph{\myindex{cofactor expansion}}.
No matter which row you pick, you always get the same number.
It is also possible to compute the
determinant by expanding
along columns (picking a column instead of a row above).
It is true that $\det(A) = \det(A^T)$.

A common notation for the determinant is a pair of vertical
lines:
\begin{equation*}
\begin{vmatrix}
a & b \\
c & d
\end{vmatrix}
=
\det \left(
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\right) .
\end{equation*}
I personally find this notation confusing as vertical lines usually
mean a positive quantity, while determinants can be negative.  Also
think about how to write the absolute value of a determinant.  I will not
use this notation in this book.

\medskip

Think of the determinants telling you the scaling of a mapping.  
If $B$ doubles the sizes of geometric objects and $A$ tripples them,
then $AB$ (which applies $B$ to an object and then $A$) should make size
go up by a factor of $6$.  This is true in general:
\begin{equation*}
\det(AB) = \det(A)\det(B) .
\end{equation*}
This property is one of the most useful, and it is employed often to 
actually compute determinants.  A particularly interesting consequence is to
note what it means for existence of inverses.
Take $A$ and $B$ to be inverses, that is $AB=I$.  Then
\begin{equation*}
\det(A)\det(B) = \det(AB) = \det(I) = 1 .
\end{equation*}
Neither $\det(A)$ nor $\det(B)$ can be zero.
Let us state this as a theorem
as it will be very important in the context of this course.

\begin{theorem}
An $n \times n$ matrix $A$ is invertible if and only if $\det (A) \not= 0$.
\end{theorem}

In fact, $\det(A^{-1}) \det(A) = 1$ says that $\det(A^{-1}) =
\frac{1}{\det(A)}$.  So we even know what the determinant of $A^{-1}$ is
before we know how to compute $A^{-1}$.

There is a simple formula for the inverse of a $2 \times 2$ matrix
\begin{equation*}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}^{-1}
=
\frac{1}{ad-bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix} .
\end{equation*}
Notice the determinant of the matrix
$[\begin{smallmatrix}a&b\\c&d\end{smallmatrix}]$
in the denominator of the fraction.
The formula only works if the determinant is nonzero, otherwise we are
dividing by zero.

FIXME


FIXME

